# Neural Network from Scratch (Python + NumPy)

Build + train a feedforward **neural network from first principles** â€” no TensorFlow/PyTorch abstractions.  
I implemented forward propagation, backpropagation, and gradient descent manually to deeply understand the math and mechanics.

> ğŸ¯ **Why this repo?**  
> It demonstrates low-level mastery of neural nets (derivatives, chain rule, vectorization) and clean engineering (modular code, plots, reproducibility).


## âœ¨ Features
- Pure **Python + NumPy** implementation (no high-level DL frameworks)
- Configurable architecture: layers, activations, learning rate, batch size, epochs
- Vectorized **forward & backward** passes
- Loss/accuracy tracking and **training curves**
- Clean project layout for quick reading


## ğŸ” Project Structure

project/
â”‚â”€â”€ README.md<br>
â”‚â”€â”€ requirements.txt<br>
â”‚â”€â”€ .gitignore<br>
â”‚â”€â”€ data/ # (optional: sample or download notes)<br>
â”‚â”€â”€ notebooks/ # (optional: exploration/training logs)<br>
â”‚â”€â”€ src/<br>
â”‚ â”œâ”€â”€ model.py # NeuralNetwork class (init, forward, backward, update)<br>
â”‚ â”œâ”€â”€ train.py # Training loop, evaluation, CLI args<br>
â”‚ â”œâ”€â”€ utils.py # Metrics, plotting, data loaders, seeds<br>
â”‚â”€â”€ results/<br>
â”‚ â”œâ”€â”€ demo.mp4 # Demo video of predictions<br>


## ğŸš€ Quick Start

```bash
pip install -r requirements.txt
python src/train.py
```

## ğŸ“Š Results

**Training Performance (batch size = 32):**
```bash
Epoch 1 â€” Loss: 0.1916, Accuracy: 0.9409
Epoch 2 â€” Loss: 0.0776, Accuracy: 0.9769
Epoch 3 â€” Loss: 0.0594, Accuracy: 0.9829
Epoch 4 â€” Loss: 0.0503, Accuracy: 0.9859
Epoch 5 â€” Loss: 0.0445, Accuracy: 0.9877
Epoch 6 â€” Loss: 0.0407, Accuracy: 0.9892
Epoch 7 â€” Loss: 0.0378, Accuracy: 0.9904
Epoch 8 â€” Loss: 0.0355, Accuracy: 0.9910
Epoch 9 â€” Loss: 0.0336, Accuracy: 0.9916
Epoch 10 â€” Loss: 0.0320, Accuracy: 0.9922
```



## ğŸ¥ Demo

[â–¶ï¸ Watch the demo video](results/demo.mp4)



